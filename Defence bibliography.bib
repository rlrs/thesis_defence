
@inproceedings{bainFrameworkBehaviouralCloning1999,
  title = {A {{Framework}} for {{Behavioural Cloning}}},
  booktitle = {Machine {{Intelligence}} 15, {{Intelligent Agents}} [{{St}}. {{Catherine}}'s {{College}}, {{Oxford}}, {{July}} 1995]},
  author = {Bain, Michael and Sammut, Claude},
  date = {1999-01-01},
  pages = {103--129},
  publisher = {{Oxford University}},
  location = {{GBR}},
  isbn = {978-0-19-853867-7}
}

@article{bareinboimPearlHierarchyFoundations,
  title = {1 {{On Pearl}}’s {{Hierarchy}} and the {{Foundations}} of {{Causal Inference}}},
  author = {Bareinboim, Elias and Correa, Juan D and Ibeling, Duligur and Icard, Thomas},
  pages = {62},
  langid = {english},
  file = {/Users/Rasmus/Zotero/storage/PBDW9MU5/Bareinboim et al. - 1 On Pearl’s Hierarchy and the Foundations of Caus.pdf}
}

@article{farquharPathspecificObjectivesSafer2022,
  title = {Path-Specific {{Objectives}} for {{Safer Agent Incentives}}},
  author = {Farquhar, Sebastian and Carey, Ryan and Everitt, Tom},
  date = {2022},
  journaltitle = {AAAI 2022},
  pages = {10},
  abstract = {We present a general framework for training safe agents whose naive incentives are unsafe. As an example, manipulative or deceptive behaviour can improve rewards but should be avoided. Most approaches fail here: agents maximize expected return by any means necessary. We formally describe settings with ‘delicate’ parts of the state which should not be used as a means to an end. We then train agents to maximize the causal effect of actions on the expected return which is not mediated by the delicate parts of state, using Causal Influence Diagram analysis. The resulting agents have no incentive to control the delicate state. We further show how our framework unifies and generalizes existing proposals.},
  langid = {english},
  file = {/Users/Rasmus/Zotero/storage/DN5ETPND/Farquhar et al. - Path-specific Objectives for Safer Agent Incentive.pdf}
}

@unpublished{hambroInsightsNeurIPS20212022,
  title = {Insights {{From}} the {{NeurIPS}} 2021 {{NetHack Challenge}}},
  author = {Hambro, Eric and Mohanty, Sharada and Babaev, Dmitrii and Byeon, Minwoo and Chakraborty, Dipam and Grefenstette, Edward and Jiang, Minqi and Jo, Daejin and Kanervisto, Anssi and Kim, Jongmin and Kim, Sungwoong and Kirk, Robert and Kurin, Vitaly and Küttler, Heinrich and Kwon, Taehwon and Lee, Donghoon and Mella, Vegard and Nardelli, Nantas and Nazarov, Ivan and Ovsov, Nikita and Parker-Holder, Jack and Raileanu, Roberta and Ramanauskas, Karolis and Rocktäschel, Tim and Rothermel, Danielle and Samvelyan, Mikayel and Sorokin, Dmitry and Sypetkowski, Maciej and Sypetkowski, Michał},
  date = {2022-03-22},
  eprint = {2203.11889},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this report, we summarize the takeaways from the first NeurIPS 2021 NetHack Challenge. Participants were tasked with developing a program or agent that can win (i.e., 'ascend' in) the popular dungeon-crawler game of NetHack by interacting with the NetHack Learning Environment (NLE), a scalable, procedurally generated, and challenging Gym environment for reinforcement learning (RL). The challenge showcased community-driven progress in AI with many diverse approaches significantly beating the previously best results on NetHack. Furthermore, it served as a direct comparison between neural (e.g., deep RL) and symbolic AI, as well as hybrid systems, demonstrating that on NetHack symbolic bots currently outperform deep RL by a large margin. Lastly, no agent got close to winning the game, illustrating NetHack's suitability as a long-term benchmark for AI research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Symbolic Computation,Statistics - Machine Learning},
  file = {/Users/Rasmus/Zotero/storage/JP5PDRQA/Hambro et al. - 2022 - Insights From the NeurIPS 2021 NetHack Challenge.pdf;/Users/Rasmus/Zotero/storage/XPR5KMYZ/2203.html}
}

@article{herlauReinforcementLearningCausal2022,
  title = {Reinforcement {{Learning}} of {{Causal Variables}} Using {{Mediation Analysis}}},
  author = {Herlau, Tue and Larsen, Rasmus},
  date = {2022},
  journaltitle = {AAAI 2022},
  pages = {8},
  abstract = {We consider the problem of acquiring causal representations and concepts in a reinforcement learning setting. Our approach defines a causal variable as being both manipulable by a policy, and able to predict the outcome. We thereby obtain a parsimonious causal graph in which interventions occur at the level of policies. The approach avoids defining a generative model of the data, prior pre-processing, or learning the transition kernel of the Markov decision process. Instead, causal variables and policies are determined by maximizing a new optimization target inspired by mediation analysis, which differs from the expected return. The maximization is accomplished using a generalization of Bellman’s equation which is shown to converge, and the method finds meaningful causal representations in a simulated environment.},
  langid = {english},
  file = {/Users/Rasmus/Zotero/storage/SUJ8ZA9Y/Herlau and Larsen - Reinforcement Learning of Causal Variables using M.pdf}
}

@article{hoValueAbstraction2019,
  title = {The Value of Abstraction},
  author = {Ho, Mark K and Abel, David and Griffiths, Thomas L and Littman, Michael L},
  date = {2019-10-01},
  journaltitle = {Current Opinion in Behavioral Sciences},
  shortjournal = {Current Opinion in Behavioral Sciences},
  series = {Artificial {{Intelligence}}},
  volume = {29},
  pages = {111--116},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2019.05.001},
  abstract = {In neuroscience, attention has been shown to bidirectionally interact with reinforcement learning (RL) to reduce the dimensionality of task representations, restricting computations to relevant features. In machine learning, despite their popularity, attention mechanisms have seldom been administered to decision-making problems. Here, we leverage a theoretical model from computational neuroscience – the attention-weighted RL (AWRL), defining how humans identify task-relevant features (i.e., that allow value predictions) – to design an applied deep RL paradigm. We formally demonstrate that the conjunction of the self-attention mechanism, widely employed in machine learning, with value function approximation is a general formulation of the AWRL model. To evaluate our agent, we train it on three Atari tasks at different complexity levels, incorporating both task-relevant and irrelevant features. Because the model uses semantic observations, we can uncover not only which features the agent elects to base decisions on, but also how it chooses to compile more complex, relational features from simpler ones. We first show that performance depends in large part on the ability to compile new compound features, rather than mere focus on individual features. In line with neuroscience predictions, self-attention leads to high resiliency to noise (irrelevant features) compared to other benchmark models. Finally, we highlight the importance and separate contributions of both bottom-up and top-down attention in the learning process. Together, these results demonstrate the broader validity of the AWRL framework in complex task scenarios, and illustrate the benefits of a deeper integration between neuroscience-derived models and RL for decision making in machine learning.},
  langid = {english},
  file = {/Users/Rasmus/Zotero/storage/LBBBNK2T/Ho et al. - 2019 - The value of abstraction.pdf;/Users/Rasmus/Zotero/storage/AT7FB2Y9/S2352154619300026.html}
}

@inproceedings{larsenProgrammaticPolicyExtraction2022,
  title = {Programmatic {{Policy Extraction}} by {{Iterative Local Search}}},
  booktitle = {Inductive {{Logic Programming}}},
  author = {Larsen, Rasmus and Schmidt, Mikkel Nørgaard},
  editor = {Katzouris, Nikos and Artikis, Alexander},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {156--166},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-97454-1_11},
  abstract = {Reinforcement learning policies are often represented by neural networks, but programmatic policies are preferred in some cases because they are more interpretable, amenable to formal verification, or generalize better. While efficient algorithms for learning neural policies exist, learning programmatic policies is challenging. Combining imitation-projection and dataset aggregation with a local search heuristic, we present a simple and direct approach to extracting a programmatic policy from a pretrained neural policy. After examining our local search heuristic on a programming by example problem, we demonstrate our programmatic policy extraction method on a pendulum swing-up problem. Both when trained using a hand crafted expert policy and a learned neural policy, our method discovers simple and interpretable policies that perform almost as well as the original.},
  isbn = {978-3-030-97454-1},
  langid = {english},
  keywords = {Hindley-Milner type system,Neighborhood search,Program synthesis,Reinforcement learning},
  file = {/Users/Rasmus/Zotero/storage/EK238PHC/Larsen and Schmidt - 2022 - Programmatic Policy Extraction by Iterative Local .pdf}
}

@book{pearlBookWhyNew2018,
  title = {The {{Book}} of {{Why}}: {{The New Science}} of {{Cause}} and {{Effect}}},
  shorttitle = {The {{Book}} of {{Why}}},
  author = {Pearl, Judea and Mackenzie, Dana},
  date = {2018-05-15},
  publisher = {{Hachette UK}},
  abstract = {A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence "Correlation is not causation." This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality -- the study of cause and effect -- on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.},
  isbn = {978-0-465-09761-6},
  langid = {english},
  pagetotal = {423},
  keywords = {Business & Economics / Statistics,Computers / Artificial Intelligence / General,Computers / Computer Science,Mathematics / Probability & Statistics / General,Science / Applied Sciences}
}

@book{pearlCausality2009a,
  title = {Causality},
  author = {Pearl, Judea},
  date = {2009-09-14},
  publisher = {{Cambridge University Press}},
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 5,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
  isbn = {978-0-521-89560-6},
  langid = {english},
  pagetotal = {487},
  keywords = {Computers / Artificial Intelligence / General,Mathematics / History & Philosophy,Philosophy / Epistemology,Philosophy / Movements / Analytic,Science / Philosophy & Social Aspects,Social Science / Research}
}

@inproceedings{pearlDirectIndirectEffects2001,
  title = {Direct and Indirect Effects},
  booktitle = {Proceedings of the {{Seventeenth}} Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Pearl, Judea},
  date = {2001-08-02},
  series = {{{UAI}}'01},
  pages = {411--420},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  location = {{San Francisco, CA, USA}},
  abstract = {The direct effect of one event on another can be defined and measured by holding constant all intermediate variables between the two. Indirect effects present conceptual and practical difficulties (in nonlinear models), because they cannot be isolated by holding certain variables constant. This paper presents a new way of defining the effect transmitted through a restricted set of paths, without controlling variables on the remaining paths. This permits the assessment of a more natural type of direct and indirect effects, one that is applicable in both linear and nonlinear models and that has broader policy-related interpretations. The paper establishes conditions under which such assessments can be estimated consistently from experimental and nonexperimental data, and thus extends path-analytic techniques to nonlinear and nonparametric models.},
  isbn = {978-1-55860-800-9},
  file = {/Users/Rasmus/Zotero/storage/LSCQ5JW6/Pearl - 2001 - Direct and indirect effects.pdf}
}

@book{pierceTypesProgrammingLanguages2002,
  title = {Types and {{Programming Languages}}},
  author = {Pierce, Benjamin C.},
  date = {2002-02-01},
  edition = {1st edition},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Mass}},
  isbn = {978-0-262-16209-8},
  langid = {english},
  pagetotal = {648}
}

@inproceedings{rossReductionImitationLearning2011,
  title = {A {{Reduction}} of {{Imitation Learning}} and {{Structured Prediction}} to {{No-Regret Online Learning}}},
  booktitle = {Proceedings of the {{Fourteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ross, Stephane and Gordon, Geoffrey and Bagnell, Drew},
  date = {2011-06-14},
  pages = {627--635},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
  langid = {english},
  file = {/Users/Rasmus/Zotero/storage/YTXG7HH4/Ross et al. - 2011 - A Reduction of Imitation Learning and Structured P.pdf}
}

@inproceedings{vermaProgrammaticallyInterpretableReinforcement2018,
  title = {Programmatically {{Interpretable Reinforcement Learning}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
  date = {2018-07-03},
  pages = {5045--5054},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural “oracle”. We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/Rasmus/Zotero/storage/8SWPM525/Verma et al. - 2018 - Programmatically Interpretable Reinforcement Learn.pdf;/Users/Rasmus/Zotero/storage/NTG85EFZ/Verma et al. - 2018 - Programmatically Interpretable Reinforcement Learn.pdf}
}


